# NutriAI: Plataforma Inteligente de Nutri√ß√£o e Receitas

[![Status do Build](https://img.shields.io/badge/build-passing-brightgreen)](https://github.com/Kamuratt/NutriIA)
[![Tecnologia](https://img.shields.io/badge/powered%20by-Docker-blue?logo=docker)](https://www.docker.com/)

**NutriAI** √© uma plataforma de engenharia de dados de ponta a ponta, projetada para transformar a maneira como as pessoas interagem com a culin√°ria e a nutri√ß√£o. O sistema automatiza a coleta, estrutura√ß√£o, enriquecimento e disponibiliza√ß√£o de receitas brasileiras, criando um ativo de dados √∫nico e de alto valor para aplica√ß√µes inteligentes.

---

## O Problema

No cen√°rio digital atual, as ferramentas de receitas e nutri√ß√£o s√£o frequentemente gen√©ricas e fragmentadas, resultando em uma experi√™ncia de usu√°rio insatisfat√≥ria:

-   **Conte√∫do N√£o-Localizado:** A maioria das bases de dados de receitas √© internacional, ignorando a riqueza da cultura, dos ingredientes e dos sabores do Brasil.
-   **An√°lise Nutricional Manual:** Receitas online raramente incluem informa√ß√µes nutricionais precisas, for√ßando usu√°rios com metas de sa√∫de a realizar c√°lculos manuais tediosos e propensos a erros.
-   **Desperd√≠cio de Alimentos:** A dificuldade em encontrar receitas com base nos ingredientes j√° dispon√≠veis em casa leva ao descarte de alimentos e ao desperd√≠cio de dinheiro.
-   **Busca Simplista:** As ferramentas de busca de receitas ainda s√£o baseadas em palavras-chave, sem uma compreens√£o real do contexto, das prefer√™ncias ou das restri√ß√µes do usu√°rio.

## A Solu√ß√£o Proposta

NutriAI aborda esses problemas atrav√©s de um pipeline de dados automatizado e uma arquitetura de microsservi√ßos desacoplada, entregando dados de alta qualidade via API para aplica√ß√µes inteligentes.

1.  **Coleta Automatizada (Web Scraping):** Um scraper robusto em Python varre fontes populares de receitas brasileiras para construir um *data lake* de pratos aut√™nticos e relevantes.
2.  **Estrutura√ß√£o Inteligente (LLM Parsing):** Modelos de Linguagem de Grande Porte (LLMs) processam o texto bruto de cada receita, extraindo ingredientes, quantidades, unidades e passos de preparo em um formato JSON padronizado e limpo.
3.  **Enriquecimento de Dados (An√°lise Nutricional):** Um script cruza os ingredientes extra√≠dos com a **Tabela Brasileira de Composi√ß√£o de Alimentos (TACO)** para calcular, com alta precis√£o, o perfil nutricional completo de cada prato (calorias, macronutrientes, etc.).
4.  **Servi√ßo e Visualiza√ß√£o (API & Frontend):** Uma API RESTful exp√µe essa base de dados enriquecida, permitindo que aplica√ß√µes, como o nosso dashboard interativo em Streamlit, fa√ßam consultas complexas e inteligentes.

---

## Arquitetura do Sistema

O projeto √© orquestrado em uma arquitetura de microsservi√ßos gerenciada via Docker Compose. Essa abordagem garante que cada componente seja independente, escal√°vel e f√°cil de manter.

```mermaid
graph TD
    subgraph "Fontes Externas"
        A["Sites de Receitas Brasileiras"]
        B["API de LLM <br>(Google Gemini/OpenAI)"]
    end

    subgraph "Pipeline de Dados (Orquestrado por n8n)"
        C["1. Scraper <br>(Python/Cloudscraper)"]
        D["2. Estrutura√ß√£o <br>(Chamada √† LLM)"]
        E["3. Enriquecimento Nutricional <br>(Script Python + Tabela TACO)"]
    end

    subgraph "Plataforma NutriAI"
        F["Banco de Dados <br>(PostgreSQL)"]
        G["API <br>(FastAPI)"]
        H["Aplica√ß√£o Web <br>(Streamlit)"]
        I["Orquestrador <br>(n8n)"]
    end
    
    J["Usu√°rio Final"]

    A --> C
    C --> D
    B --> D
    D --> E
    E --> F
    F <--> G
    G <--> H
    J <--> H
    I -- "Gerencia e agenda" --> C
```

O fluxo de dados √© totalmente automatizado: o **n8n** agenda e executa o **scraper**, que coleta os dados brutos. Em seguida, o mesmo workflow orquestra a chamada √† **API da LLM** para estrutura√ß√£o e ao script de **enriquecimento nutricional**, persistindo o resultado final no banco de dados **PostgreSQL**. A **API FastAPI** serve esses dados para a aplica√ß√£o **Streamlit**, onde o usu√°rio pode interagir com as receitas.

---

## Stack Tecnol√≥gico

A sele√ß√£o de tecnologias foi feita para garantir performance, escalabilidade e uma excelente experi√™ncia de desenvolvimento.

-   **Backend & API:** **FastAPI**
    -   *Por qu√™?* Pela sua alta performance ass√≠ncrona, valida√ß√£o de dados nativa com Pydantic e gera√ß√£o autom√°tica de documenta√ß√£o interativa (Swagger UI).
-   **Frontend:** **Streamlit**
    -   *Por qu√™?* Permite a prototipagem e constru√ß√£o r√°pida de aplica√ß√µes de dados interativas com Python puro, ideal para visualiza√ß√£o e intera√ß√£o com a API.
-   **Orquestra√ß√£o de Workflows:** **n8n**
    -   *Por qu√™?* Uma ferramenta *low-code* poderosa para automa√ß√£o de pipelines. Permite visualizar, agendar e gerenciar o fluxo de dados de forma intuitiva.
-   **Banco de Dados:** **PostgreSQL**
    -   *Por qu√™?* Um banco de dados relacional robusto, confi√°vel e com excelente suporte para tipos de dados complexos como JSON, ideal para armazenar as receitas estruturadas.
-   **Intelig√™ncia Artificial:** **APIs do Google Gemini / OpenAI**
    -   *Por qu√™?* Modelos de ponta para tarefas de Processamento de Linguagem Natural, capazes de extrair informa√ß√µes de texto n√£o-estruturado com alta precis√£o.
-   **Infraestrutura:** **Docker & Docker Compose**
    -   *Por qu√™?* Para criar um ambiente de desenvolvimento e produ√ß√£o consistente, reprodut√≠vel e isolado, simplificando o setup e o deploy.

---

## Estrutura do Projeto

O reposit√≥rio est√° organizado da seguinte forma para manter a clareza e a separa√ß√£o de responsabilidades:

```
.
‚îú‚îÄ‚îÄ api/             # L√≥gica do backend e servi√ßo da API com FastAPI
‚îú‚îÄ‚îÄ data/            # Arquivos de dados, como a Tabela TACO processada
‚îú‚îÄ‚îÄ n8n/             # Configura√ß√µes e workflows do n8n
‚îú‚îÄ‚îÄ scripts/         # Scripts independentes (scraper, enriquecimento, etc.)
‚îú‚îÄ‚îÄ streamlit-app/   # C√≥digo da aplica√ß√£o frontend com Streamlit
‚îú‚îÄ‚îÄ .env.example     # Template para vari√°veis de ambiente
‚îú‚îÄ‚îÄ docker-compose.yml # Orquestra√ß√£o de todos os servi√ßos
‚îî‚îÄ‚îÄ README.md        # Esta documenta√ß√£o
```

---

## Guia de Instala√ß√£o e Uso

O projeto √© 100% conteinerizado. Siga os passos abaixo para executar a plataforma completa localmente.

1.  **Pr√©-requisitos:**
    * [Docker](https://www.docker.com/get-started) e [Docker Compose](https://docs.docker.com/compose/install/) instalados.
    * [Git](https://git-scm.com/) para clonar o reposit√≥rio.

2.  **Clonagem do Reposit√≥rio:**
    ```bash
    git clone [https://github.com/Kamuratt/NutriIA.git](https://github.com/Kamuratt/NutriIA.git)
    cd NutriIA
    ```

3.  **Configura√ß√£o do Ambiente:**
    * Crie uma c√≥pia do arquivo de exemplo `.env.example` e renomeie para `.env`.
    * Edite o arquivo `.env` e preencha as vari√°veis de ambiente, especialmente sua chave de API para a LLM (`GEMINI_API_KEY`).

4.  **Execu√ß√£o da Plataforma:**
    ```bash
    docker-compose up --build
    ```
    Este comando ir√° construir as imagens Docker (na primeira vez) e iniciar todos os cont√™ineres de forma integrada.

5.  **Acesso aos Servi√ßos:**
    * **Aplica√ß√£o Web (Frontend):** `http://localhost:8501`
    * **Documenta√ß√£o da API (Swagger):** `http://localhost:8000/docs`
    * **Painel de Automa√ß√£o (n8n):** `http://localhost:5678`

---

## Roadmap de Desenvolvimento

Nosso plano de desenvolvimento est√° focado em transformar o prot√≥tipo funcional em um produto de dados robusto, confi√°vel e com funcionalidades inteligentes.

### Fase 1: Funda√ß√£o e MVP (Conclu√≠da ‚úÖ)
-   [x] Desenvolvimento do Scraper e pipeline de dados inicial.
-   [x] Integra√ß√£o com LLM para estrutura√ß√£o de receitas.
-   [x] Implementa√ß√£o da API base e do banco de dados.
-   [x] Conteineriza√ß√£o completa da aplica√ß√£o com Docker.
-   [x] Automa√ß√£o do pipeline com workflows no n8n.
-   [x] Cria√ß√£o de um frontend interativo com Streamlit.

### Fase 2: Robustez e Qualidade de Dados (Foco Atual üéØ)
O objetivo desta fase √© tornar o pipeline √† prova de falhas e garantir a m√°xima qualidade e consist√™ncia dos dados.
-   [ ] **Valida√ß√£o de Dados:** Implementar `Pydantic` de forma estrita na API para garantir a integridade dos dados que entram e saem do sistema.
-   [ ] **Logging Estruturado:** Substituir `print()` por um sistema de logging robusto (ex: m√≥dulo `logging` do Python) para monitorar e depurar os servi√ßos de forma eficaz.
-   [ ] **Normaliza√ß√£o de Ingredientes:** Criar um m√≥dulo para padronizar nomes de ingredientes (ex: "tomate cereja" -> "tomate") antes da an√°lise nutricional para aumentar a precis√£o.

### Fase 3: Otimiza√ß√£o e Escalabilidade (Pr√≥ximos Passos üöÄ)
-   [ ] **Implementar Cache:** Evitar reprocessamento de receitas j√° analisadas para economizar custos de API e tempo de processamento.
-   [ ] **Desenvolver Scraping Incremental:** Refinar o scraper para buscar apenas por conte√∫do novo ou atualizado, tornando a coleta mais eficiente.
-   [ ] **Adicionar Testes Automatizados:** Implementar testes unit√°rios e de integra√ß√£o com `pytest` para garantir a estabilidade do c√≥digo.

### Fase 4: Expans√£o da Intelig√™ncia (Features Futuras üí°)
-   [ ] **M√≥dulo "Desperd√≠cio Zero":** Funcionalidade para o usu√°rio inserir os ingredientes que possui e receber um plano de refei√ß√µes otimizado.
-   [ ] **M√≥dulo "Paladar Personalizado":** Sistema de recomenda√ß√£o que aprende as prefer√™ncias do usu√°rio para sugerir novas receitas.
-   [ ] **CI/CD:** Configurar um pipeline de Integra√ß√£o e Deploy Cont√≠nuos com GitHub Actions para automatizar os testes e o deploy.
