# NutriAI: Plataforma Inteligente de Nutri√ß√£o e Receitas

[![Status do Build](https://img.shields.io/badge/build-passing-brightgreen)](https://github.com/Kamuratt/NutriIA)
[![Tecnologia](https://img.shields.io/badge/powered%20by-Docker-blue?logo=docker)](https://www.docker.com/)

**NutriAI** √© uma plataforma de engenharia de dados de ponta a ponta, projetada para transformar a maneira como as pessoas interagem com a culin√°ria e a nutri√ß√£o. O sistema automatiza a coleta, estrutura√ß√£o, enriquecimento e disponibiliza√ß√£o de receitas brasileiras, culminando em um planejador de dietas inteligente que entrega planos nutricionais personalizados em PDF.

---

## O Problema

No cen√°rio digital atual, as ferramentas de receitas e nutri√ß√£o s√£o frequentemente gen√©ricas e fragmentadas, resultando em uma experi√™ncia de usu√°rio insatisfat√≥ria:

-   **Conte√∫do N√£o-Localizado:** A maioria das bases de dados de receitas √© internacional, ignorando a riqueza da cultura, dos ingredientes e dos sabores do Brasil.
-   **An√°lise Nutricional Manual:** Receitas online raramente incluem informa√ß√µes nutricionais precisas, for√ßando usu√°rios com metas de sa√∫de a realizar c√°lculos manuais tediosos e propensos a erros.
-   **Desperd√≠cio de Alimentos:** A dificuldade em encontrar receitas com base nos ingredientes j√° dispon√≠veis em casa leva ao descarte de alimentos e ao desperd√≠cio de dinheiro.
-   **Planejamento Gen√©rico:** Ferramentas de dieta muitas vezes fornecem metas cal√≥ricas, mas deixam o usu√°rio sozinho na tarefa complexa de criar um plano de refei√ß√µes variado e que atenda a essas metas.

## A Solu√ß√£o Proposta

NutriAI aborda esses problemas atrav√©s de um pipeline de dados automatizado e uma arquitetura de microsservi√ßos desacoplada, entregando um planejador de dietas inteligente via API:

1.  **Coleta Automatizada (Web Scraping):** Um scraper robusto em Python varre fontes populares de receitas brasileiras para construir um *data lake* de pratos aut√™nticos e relevantes.
2.  **Estrutura√ß√£o Inteligente (LLM Parsing):** Modelos de Linguagem de Grande Porte (LLMs, como Google Gemini) processam o texto bruto de cada receita, extraindo ingredientes, quantidades, unidades e passos de preparo em um formato JSON padronizado e limpo.
3.  **Enriquecimento de Dados (An√°lise Nutricional):** Um script cruza os ingredientes extra√≠dos com a **Tabela Brasileira de Composi√ß√£o de Alimentos (TACO)** para calcular, com alta precis√£o, o perfil nutricional completo de cada prato (calorias, macronutrientes) e classificar restri√ß√µes alimentares (vegano, sem gl√∫ten, etc.).
4.  **Servi√ßo Inteligente (API FastAPI):** Uma API RESTful recebe os dados de sa√∫de do usu√°rio (peso, altura, objetivo, restri√ß√µes), calcula a meta cal√≥rica, consulta o banco por receitas compat√≠veis, e orquestra a gera√ß√£o do plano com a LLM.
5.  **Gera√ß√£o de Plano e Relat√≥rio (LLM + PDF):** A API instrui a LLM a atuar como nutricionista para criar um plano de refei√ß√µes semanal textual. Esse plano, junto com os detalhes das receitas sugeridas e uma **lista de compras agregada**, √© renderizado em um relat√≥rio PDF profissional usando WeasyPrint e Jinja2.
6.  **Intera√ß√£o com Usu√°rio (Frontend Streamlit):** Uma aplica√ß√£o web interativa coleta os dados do usu√°rio, envia a requisi√ß√£o para a API e disponibiliza o PDF gerado para download.

---

## Arquitetura do Sistema

O projeto √© orquestrado em uma arquitetura de microsservi√ßos gerenciada via Docker Compose. Essa abordagem garante que cada componente seja independente, escal√°vel e f√°cil de manter.

```mermaid
graph TD
    subgraph "Fontes Externas"
        A["Sites de Receitas Brasileiras"]
        B["API de LLM <br>(Google Gemini/OpenAI)"]
    end

    subgraph "Pipeline de Dados (Orquestrado por n8n)"
        C["1. Scraper <br>(Python/Cloudscraper)"]
        D["2. Estrutura√ß√£o <br>(Chamada √† LLM)"]
        E["3. Enriquecimento Nutricional <br>(Script Python + Tabela TACO)"]
    end

    subgraph "Plataforma NutriAI"
        F["Banco de Dados <br>(PostgreSQL)"]
        G["API <br>(FastAPI)"]
        H["Aplica√ß√£o Web <br>(Streamlit)"]
        I["Orquestrador <br>(n8n)"]
    end
    
    J["Usu√°rio Final"]

    A --> C
    C --> D
    B --> D
    D --> E
    E --> F
    F <--> G
    G <--> H
    J <--> H
    I -- "Gerencia e agenda" --> C

```
## O sistema possui dois fluxos principais:

1.  **Fluxo de ETL (Ass√≠ncrono, via n8n):** O **n8n** agenda e executa o **scraper** (C) para coletar dados brutos (A). Em seguida, orquestra a chamada √† **API da LLM** (B) para *estrutura√ß√£o* dos dados (D) e executa o script de **enriquecimento nutricional** (E), persistindo o resultado final no **PostgreSQL** (F). Este fluxo alimenta a base de dados.
2.  **Fluxo de Gera√ß√£o de Plano (S√≠ncrono, via Usu√°rio):** O **Usu√°rio Final** (J) interage com a **Aplica√ß√£o Web Streamlit** (H), fornecendo seus dados. O Streamlit envia uma requisi√ß√£o para a **API FastAPI** (G). A API (G) consulta o **Banco de Dados** (F) por receitas, chama novamente a **API da LLM** (B) para *gerar* o plano de refei√ß√µes, e utiliza WeasyPrint/Jinja2 para criar o PDF, que √© devolvido ao usu√°rio atrav√©s do Streamlit (H).

---

## Stack Tecnol√≥gico

A sele√ß√£o de tecnologias foi feita para garantir performance, escalabilidade e uma excelente experi√™ncia de desenvolvimento.

-   **Backend & API:** **FastAPI**
    -   *Por qu√™?* Pela sua alta performance ass√≠ncrona, valida√ß√£o de dados nativa com Pydantic e gera√ß√£o autom√°tica de documenta√ß√£o interativa (Swagger UI).
-   **Frontend:** **Streamlit**
    -   *Por qu√™?* Permite a prototipagem e constru√ß√£o r√°pida de aplica√ß√µes de dados interativas com Python puro, ideal para coletar dados do usu√°rio e apresentar o resultado (PDF).
-   **Orquestra√ß√£o de Workflows:** **n8n**
    -   *Por qu√™?* Uma ferramenta *low-code* poderosa para automa√ß√£o do pipeline de ETL. Permite visualizar, agendar e gerenciar o fluxo de dados de forma intuitiva.
-   **Banco de Dados:** **PostgreSQL**
    -   *Por qu√™?* Um banco de dados relacional robusto, confi√°vel e com excelente suporte para tipos de dados complexos como JSONB, ideal para armazenar as receitas estruturadas e seus metadados nutricionais.
-   **Intelig√™ncia Artificial:** **APIs do Google Gemini / OpenAI**
    -   *Por qu√™?* Modelos de ponta usados para duas tarefas distintas: (1) Processamento de Linguagem Natural para *estruturar* os dados brutos das receitas e (2) Gera√ß√£o de Linguagem Natural para *criar* os planos alimentares personalizados.
-   **Gera√ß√£o de Relat√≥rios:** **WeasyPrint** & **Jinja2**
    -   *Por qu√™?* `Jinja2` permite criar templates HTML sofisticados para o plano. `WeasyPrint` renderiza esses templates em arquivos PDF de alta qualidade diretamente no backend.
-   **Infraestrutura:** **Docker & Docker Compose**
    -   *Por qu√™?* Para criar um ambiente de desenvolvimento e produ√ß√£o consistente, reprodut√≠vel e isolado, simplificando o setup e o deploy.

---

## Estrutura do Projeto

### O reposit√≥rio est√° organizado da seguinte forma para manter a clareza e a separa√ß√£o de responsabilidades:
```
.
‚îú‚îÄ‚îÄ nutriai/
‚îÇ   ‚îú‚îÄ‚îÄ api/             # L√≥gica do backend com FastAPI
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ templates/   # Template Jinja2 para o PDF (plano_dieta.html)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py      # Ponto de entrada da API, define os endpoints
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py    # Defini√ß√£o das tabelas (SQLAlchemy ORM)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schemas.py   # Defini√ß√£o dos dados da API (Pydantic)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pdf_generator.py # Gera√ß√£o do PDF e lista de compras
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ crud.py      # Fun√ß√µes de acesso ao banco
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ database.py  # Configura√ß√£o da conex√£o com o banco
‚îÇ   ‚îú‚îÄ‚îÄ migration.py     # Scripts de migra√ß√£o (SQLite -> Postgres)
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ data/            # Arquivos de dados (TACO) e dumps de banco
‚îú‚îÄ‚îÄ n8n-custom/      # Configs e Dockerfile customizado do n8n
‚îú‚îÄ‚îÄ scripts/         # Scripts independentes (auditoria, c√°lculo, web_scrap, etc.)
‚îú‚îÄ‚îÄ frontend/        # C√≥digo da aplica√ß√£o frontend com Streamlit (app.py)
‚îú‚îÄ‚îÄ .env.example     # Template para vari√°veis de ambiente
‚îú‚îÄ‚îÄ docker-compose.yml # Orquestra√ß√£o de todos os servi√ßos
‚îî‚îÄ‚îÄ README.md        # Esta documenta√ß√£o
```

---

## Guia de Instala√ß√£o e Uso

O projeto √© 100% conteinerizado. Siga os passos abaixo para executar a plataforma completa localmente.

1.  **Pr√©-requisitos:**
    * [Docker](https://www.docker.com/get-started) e [Docker Compose](https://docs.docker.com/compose/install/) instalados.
    * [Git](https://git-scm.com/) para clonar o reposit√≥rio.

2.  **Clonagem do Reposit√≥rio:**
    ```bash
    git clone [https://github.com/Kamuratt/NutriIA.git](https://github.com/Kamuratt/NutriIA.git)
    cd NutriI
    ```

3.  **Configura√ß√£o do Ambiente:**
    * Crie uma c√≥pia do arquivo de exemplo `.env.example` e renomeie para `.env`.
    * Edite o arquivo `.env` e preencha as vari√°veis de ambiente, especialmente sua chave de API para a LLM (`GOOGLE_API_KEY`).

4.  **Execu√ß√£o da Plataforma:**
    ```bash
    # Este comando ir√° construir as imagens Docker (na primeira vez) e iniciar todos os cont√™ineres de forma integrada. Aguarde at√© que todos os servi√ßos estejam rodando         (        especialmente o banco de dados).

    docker-compose up --build
    ```
    
5.  **Acesso aos Servi√ßos:**
    * **Aplica√ß√£o Web (Frontend):** `http://localhost:8501`
    * **Documenta√ß√£o da API (Swagger):** `http://localhost:8085/docs`
    * **Painel de Automa√ß√£o (n8n):** `http://localhost:5678`

6.  **(Opcional) Execu√ß√£o de Scripts Manuais:**
    Para executar scripts de manuten√ß√£o (migra√ß√£o, c√°lculo de nutrientes, auditoria), utilize o servi√ßo `runner`:

    ```bash

    # Entra no shell interativo do cont√™iner 'runner'
    docker-compose exec runner bash

    # Dentro do cont√™iner, execute o script desejado:
    python scripts/calcular_nutrientes.py --mode new
    # Ou
    python nutriai/migration.py

    ```

---

## Roadmap de Desenvolvimento

Nosso plano de desenvolvimento est√° focado em transformar o prot√≥tipo funcional em um produto de dados robusto, confi√°vel e com funcionalidades inteligentes.

### Fase 1: Funda√ß√£o e MVP (Conclu√≠da ‚úÖ)
-   [x] Desenvolvimento do Scraper e pipeline de dados inicial.
-   [x] Integra√ß√£o com LLM para estrutura√ß√£o de receitas.
-   [x] Implementa√ß√£o da API base e do banco de dados (PostgreSQL).
-   [x] Conteineriza√ß√£o completa da aplica√ß√£o com Docker.
-   [x] Automa√ß√£o do pipeline de ETL com workflows no n8n.
-   [x] Cria√ß√£o de um frontend interativo com Streamlit.
-   [x] Implementa√ß√£o de endpoint de gera√ß√£o de plano de dieta (`/planejar-dieta/`) com LLM.
-   [x] Gera√ß√£o de relat√≥rios em PDF com WeasyPrint e Jinja2.
-   [x] Gera√ß√£o de lista de compras agregada e inteligente.
-   [x] Valida√ß√£o de Dados na API com Pydantic (`schemas.py`).
-   [x] C√°lculo de TMB e Meta Cal√≥rica.
-   [x] Classifica√ß√£o de restri√ß√µes alimentares nas receitas.

### Fase 2: Robustez e Qualidade de Dados (Foco Atual üéØ)
O objetivo desta fase √© tornar o pipeline √† prova de falhas e garantir a m√°xima qualidade e consist√™ncia dos dados.
-   [ ] **Logging Estruturado:** Substituir `print()` por um sistema de logging robusto (ex: m√≥dulo `logging` do Python) para monitorar e depurar os servi√ßos de forma eficaz.
-   [ ] **Normaliza√ß√£o de Ingredientes (ETL):** Expandir o m√≥dulo de normaliza√ß√£o (usado na lista de compras) para ser aplicado *antes* da an√°lise nutricional no ETL, aumentando a precis√£o dos c√°lculos.
-   [ ] **Implementar Cache (ETL):** Evitar reprocessamento de receitas j√° estruturadas pela LLM para economizar custos de API e tempo.

### Fase 3: Otimiza√ß√£o e Escalabilidade (Pr√≥ximos Passos üöÄ)
-   [ ] **Desenvolver Scraping Incremental:** Refinar o scraper para buscar apenas por conte√∫do novo ou atualizado, tornando a coleta mais eficiente.
-   [ ] **Adicionar Testes Automatizados:** Implementar testes unit√°rios e de integra√ß√£o com `pytest` para garantir a estabilidade do c√≥digo, especialmente da API e das l√≥gicas de c√°lculo/gera√ß√£o.
-   [ ] **CI/CD:** Configurar um pipeline de Integra√ß√£o e Deploy Cont√≠nuos com GitHub Actions para automatizar os testes e o deploy.

### Fase 4: Expans√£o da Intelig√™ncia (Features Futuras üí°)
-   [ ] **M√≥dulo "Desperd√≠cio Zero":** Funcionalidade para o usu√°rio inserir os ingredientes que possui e receber sugest√µes de receitas ou um plano de refei√ß√µes otimizado (o inverso da lista de compras atual).
-   [ ] **M√≥dulo "Paladar Personalizado":** Sistema de recomenda√ß√£o que aprende as prefer√™ncias do usu√°rio (receitas favoritas, ingredientes evitados) para sugerir novas receitas e aprimorar a gera√ß√£o do plano.
-   [ ] **Interface de Edi√ß√£o/Feedback:** Permitir que o usu√°rio edite o plano gerado ou d√™ feedback sobre as sugest√µes para refinar futuras gera√ß√µes.
